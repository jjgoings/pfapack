name: Build and Test

on:
  push:
  pull_request:
  workflow_dispatch:
    inputs:
      benchmark_ref:
        description: 'Git ref (commit SHA) to benchmark'
        required: false
        default: ''

jobs:
  build_and_test:
    runs-on: ${{ matrix.os }}
    if: github.event.inputs.benchmark_ref == '' || github.event.inputs.benchmark_ref == null
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.10', '3.11', '3.12']

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gfortran liblapack-dev libblas-dev

    - name: Set compiler environment variables
      run: |
        echo "CC=gcc" >> $GITHUB_ENV
        echo "FC=gfortran" >> $GITHUB_ENV

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Install package
      run: pip install .

    - name: Build C/Fortran extensions
      run: python setup.py build_ext --inplace

    - name: Run tests
      run: pytest -v tests

  benchmark:
    runs-on: ${{ matrix.os }}
    needs: build_and_test
    if: |
      always() &&
      (needs.build_and_test.result == 'success' || needs.build_and_test.result == 'skipped') &&
      ((github.event_name == 'push' && github.ref == 'refs/heads/main') ||
       (github.event_name == 'workflow_dispatch'))
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]

    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.event.inputs.benchmark_ref || github.sha }}

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install system dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gfortran liblapack-dev libblas-dev

    - name: Install system dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install gcc
        # Create symlinks so 'gfortran' and 'gcc' work (Makefiles use these names)
        sudo ln -sf $(which gfortran-14) /usr/local/bin/gfortran
        sudo ln -sf $(which gcc-14) /usr/local/bin/gcc

    - name: Set compiler and threading environment
      run: |
        echo "OMP_NUM_THREADS=1" >> $GITHUB_ENV
        echo "OPENBLAS_NUM_THREADS=1" >> $GITHUB_ENV
        echo "VECLIB_MAXIMUM_THREADS=1" >> $GITHUB_ENV
        if [ "$RUNNER_OS" == "Linux" ]; then
          echo "CC=gcc" >> $GITHUB_ENV
          echo "FC=gfortran" >> $GITHUB_ENV
        else
          echo "CC=gcc-14" >> $GITHUB_ENV
          echo "FC=gfortran-14" >> $GITHUB_ENV
        fi

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Install package
      run: pip install .

    - name: Build C/Fortran extensions
      run: python setup.py build_ext --inplace

    - name: Run benchmarks
      run: |
        python -c "
        import json
        import time
        import numpy as np

        # Try different import paths for compatibility with old commits
        pf_ctypes = None
        pf_batched_4d = None

        try:
            from pfapack.ctypes import pfaffian as pf_ctypes
        except ImportError:
            pass

        try:
            from pfapack.ctypes import pfaffian_batched_4d as pf_batched_4d
        except ImportError:
            pass

        if pf_ctypes is None:
            try:
                from pfapack import pfaffian as pf_mod
                pf_ctypes = pf_mod.pfaffian
            except:
                pass

        results = {'benchmarks': []}
        np.random.seed(42)

        # Benchmark config (matches current CI settings)
        n, outer, inner = 16, 5, 3
        n_iters = 100

        # Generate test matrix
        A_batch = np.random.randn(outer, inner, n, n) + 1j*np.random.randn(outer, inner, n, n)
        A_batch = A_batch - A_batch.transpose(0, 1, 3, 2)

        # c_loop benchmark (individual pfaffian calls)
        if pf_ctypes:
            times = []
            for _ in range(n_iters):
                t0 = time.perf_counter()
                for i in range(outer):
                    for j in range(inner):
                        pf_ctypes(A_batch[i,j].copy())
                times.append(time.perf_counter() - t0)
            results['benchmarks'].append({
                'name': 'c_loop',
                'stats': {'mean': np.mean(times), 'stddev': np.std(times),
                         'min': np.min(times), 'max': np.max(times)}
            })
            print(f'c_loop: {np.mean(times)*1000:.3f} ms')

        # batched_4d benchmark (if available)
        if pf_batched_4d:
            times = []
            for _ in range(n_iters):
                t0 = time.perf_counter()
                pf_batched_4d(A_batch.copy())
                times.append(time.perf_counter() - t0)
            results['benchmarks'].append({
                'name': 'batched_4d',
                'stats': {'mean': np.mean(times), 'stddev': np.std(times),
                         'min': np.min(times), 'max': np.max(times)}
            })
            print(f'batched_4d: {np.mean(times)*1000:.3f} ms')

        with open('benchmark-pytest.json', 'w') as f:
            json.dump(results, f)

        if not results['benchmarks']:
            print('WARNING: No benchmarks could run')
            exit(1)
        "

    - name: Run phase2 benchmark (tomography)
      continue-on-error: true
      run: |
        if [ -f bench/phase2_bench.py ]; then
          python bench/phase2_bench.py tomography \
            --iters 5 --warmup 2 \
            --nshadow 128 --ngrid 17 --n-sel 32 --n-full 64 \
            --json-out benchmark-tomography.json
        else
          echo '{"runs": []}' > benchmark-tomography.json
        fi

    - name: Run phase2 benchmark (many-small)
      continue-on-error: true
      run: |
        if [ -f bench/phase2_bench.py ]; then
          python bench/phase2_bench.py many-small \
            --iters 5 --warmup 2 \
            --outer 65536 --inner 5 --n 8 \
            --json-out benchmark-many-small.json
        else
          echo '{"runs": []}' > benchmark-many-small.json
        fi

    - name: Convert benchmarks to unified format
      run: |
        python -c "
        import json
        from pathlib import Path

        results = []

        # pytest-benchmark results
        pb = json.loads(Path('benchmark-pytest.json').read_text())
        for bench in pb.get('benchmarks', []):
            results.append({
                'name': bench['name'],
                'unit': 's',
                'value': bench['stats']['mean'],
                'range': f\"± {bench['stats']['stddev']:.6f}\",
                'extra': f\"min={bench['stats']['min']:.6f} max={bench['stats']['max']:.6f}\"
            })

        # phase2 tomography
        tomo = json.loads(Path('benchmark-tomography.json').read_text())
        for run in tomo.get('runs', []):
            results.append({
                'name': f\"phase2_{run['meta']['workload']}\",
                'unit': 's',
                'value': run['stats']['mean'],
                'range': f\"± {run['stats']['stdev']:.6f}\",
                'extra': f\"n_sel={run['meta'].get('n_sel', 'N/A')}\"
            })

        # phase2 many-small
        small = json.loads(Path('benchmark-many-small.json').read_text())
        for run in small.get('runs', []):
            results.append({
                'name': f\"phase2_{run['meta']['workload']}\",
                'unit': 's',
                'value': run['stats']['mean'],
                'range': f\"± {run['stats']['stdev']:.6f}\",
                'extra': f\"outer={run['meta'].get('outer', 'N/A')} n={run['meta'].get('n', 'N/A')}\"
            })

        Path('benchmark-results.json').write_text(json.dumps(results, indent=2))
        print(json.dumps(results, indent=2))
        "

    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        name: PFAPACK Performance (${{ matrix.os }})
        tool: 'customSmallerIsBetter'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        alert-threshold: '150%'
        comment-on-alert: true
        fail-on-alert: false
        gh-pages-branch: gh-pages
        benchmark-data-dir-path: dev/bench/${{ matrix.os }}

    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}
        path: |
          benchmark-pytest.json
          benchmark-tomography.json
          benchmark-many-small.json
          benchmark-results.json

  benchmark_pr:
    runs-on: ${{ matrix.os }}
    needs: build_and_test
    if: github.event_name == 'pull_request'
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install system dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential gfortran liblapack-dev libblas-dev

    - name: Install system dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install gcc
        # Create symlinks so 'gfortran' and 'gcc' work (Makefiles use these names)
        sudo ln -sf $(which gfortran-14) /usr/local/bin/gfortran
        sudo ln -sf $(which gcc-14) /usr/local/bin/gcc

    - name: Set compiler and threading environment
      run: |
        echo "OMP_NUM_THREADS=1" >> $GITHUB_ENV
        echo "OPENBLAS_NUM_THREADS=1" >> $GITHUB_ENV
        echo "VECLIB_MAXIMUM_THREADS=1" >> $GITHUB_ENV
        if [ "$RUNNER_OS" == "Linux" ]; then
          echo "CC=gcc" >> $GITHUB_ENV
          echo "FC=gfortran" >> $GITHUB_ENV
        else
          echo "CC=gcc-14" >> $GITHUB_ENV
          echo "FC=gfortran-14" >> $GITHUB_ENV
        fi

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Install package
      run: pip install .

    - name: Build C/Fortran extensions
      run: python setup.py build_ext --inplace

    - name: Run pytest benchmarks
      run: |
        pytest tests/test_performance_timings.py \
          --benchmark-json=benchmark-pytest.json \
          --benchmark-min-rounds=5

    - name: Run phase2 benchmarks
      run: |
        python bench/phase2_bench.py tomography \
          --iters 3 --warmup 1 \
          --nshadow 128 --ngrid 17 --n-sel 32 --n-full 64 \
          --json-out benchmark-tomography.json
        python bench/phase2_bench.py many-small \
          --iters 3 --warmup 1 \
          --outer 65536 --inner 5 --n 8 \
          --json-out benchmark-many-small.json

    - name: Report benchmark summary
      run: |
        echo "## Benchmark Results (${{ matrix.os }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python -c "
        import json
        from pathlib import Path

        print('### pytest-benchmark')
        pb = json.loads(Path('benchmark-pytest.json').read_text())
        print('| Test | Mean | StdDev |')
        print('|------|------|--------|')
        for b in pb.get('benchmarks', []):
            name = b['name'].split('[')[0]
            print(f\"| {name} | {b['stats']['mean']*1000:.3f} ms | {b['stats']['stddev']*1000:.3f} ms |\")

        print()
        print('### phase2 benchmarks')
        print('| Workload | Mean | StdDev |')
        print('|----------|------|--------|')
        for f in ['benchmark-tomography.json', 'benchmark-many-small.json']:
            data = json.loads(Path(f).read_text())
            for run in data.get('runs', []):
                wl = run['meta']['workload']
                print(f\"| {wl} | {run['stats']['mean']*1000:.3f} ms | {run['stats']['stdev']*1000:.3f} ms |\")
        " >> $GITHUB_STEP_SUMMARY

    - name: Upload benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-pr-${{ matrix.os }}
        path: |
          benchmark-pytest.json
          benchmark-tomography.json
          benchmark-many-small.json
